# Example .env for DeepSeek OCR
# Copy this to .env and fill in values for your environment

# VLLM Service (Model path mapping)

# MODEL_HOST_PATH should point to the model directory on the HOST machine
# MODEL_PATH will be set to MODEL_CONTAINER_PATH inside the container (used by the app)
####################################### IMPORTANT: Change MODEL_HOST_PATH to the actual path on your host machine#
#
MODEL_HOST_PATH=./models      # <-- CHANGE THIS !! ##########################################
#
##################################################################################################################




# Bellow are default settings which can be changed as needed

# MODEL_CONTAINER_PATH is where the model will be mounted inside the container
MODEL_CONTAINER_PATH=/models/DeepSeek-OCR

# VLLM server
VLLM_PORT=9000
VLLM_HOST=0.0.0.0

# Application (FastAPI) host/port the Python app reads as `HOST` and `PORT`.
# For most users running with docker-compose you don't need to change these.
# The app's `Vllm/config.py` reads `HOST`/`PORT` (defaults below). If you prefer
# to use the VLLM_* names keep them as-is; both are informational here.
HOST=0.0.0.0
PORT=9000

# GPU/ROCm
HIP_VISIBLE_DEVICES=0
CUDA_VISIBLE_DEVICES=
PYTORCH_ROCM_ARCH=gfx1100
VLLM_WORKER_MULTIPROC_METHOD=spawn

# Frontend
FRONTEND_HOST=0.0.0.0
FRONTEND_PORT=5173

# Vite build-time variables
VITE_API_BASE_URL=/api
VITE_APP_VERSION=1.0.0
VITE_ENABLE_WEBCAM=true
VITE_ENABLE_FILE_UPLOAD=true

# Nginx
NGINX_PORT=8080
